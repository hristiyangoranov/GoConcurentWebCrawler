Starting from provided web page URL/s (given as command line arguments), crawl web pages recursively, extracting all connected web resource links, and crawling that linked resources in worker goroutines (reused - from a worker pool). A command argument flag should be given whether to follow external links (different from cdns serving online versions of JS and CSS libraries).
The communication between goroutines should be implemented using channels.
The program should accept a crawling (page traversal and indexing) timeout parameter as additional optional command line argument (2 min by default, e.g. using: context.WithTimeout(context.Background(), 2*time.Minute) for that purpose, as in the examples discussed).
The maximum number of goroutines created any given moment should not exceed the predefined constant in the program, or can be given too as additional command line argument.
For each web-site resource crawled the program should extract used images (raster, as well vector images â€“ SVG). The image thumbnails (200px max-width) should be saved  in a file system directory. For each image there should be created a record in MySQL database containing image metadata such as image thumbnail, url, filename, alternative text, title, resolution, format, etc.
The results from crawling and extracting images from the web-site resources should allow searching and visualization of images  by a combination of search parameters (image metadata parameters) using a web interface (image search page) implemented using golang HTML templates.# GoConcurentWebCrawler
